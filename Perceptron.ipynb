{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions to read and extract data in desired format\n",
    "def read(dataset = \"training\", path = \".\"):\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        print(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        _, __, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows * cols)\n",
    "\n",
    "    get_img = lambda index: (lbl[index], img[index])\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_img(i)\n",
    "\n",
    "def show(image):\n",
    "    from matplotlib import pyplot\n",
    "    import matplotlib as mpl\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image.reshape(28, 28), cmap=mpl.cm.gray)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the images\n",
    "TRAIN = read('training', 'MNIST'); TEST = read('testing', 'MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = []; lbl_train = []\n",
    "img_test = []; lbl_test = []\n",
    "\n",
    "for temp in TRAIN:\n",
    "    img_train.append(temp[1])\n",
    "    lbl_train.append(temp[0])\n",
    "\n",
    "for temp in TEST:\n",
    "    img_test.append(temp[1])\n",
    "    lbl_test.append(temp[0])\n",
    "\n",
    "img_train = np.array(img_train);lbl_train = np.array(lbl_train)\n",
    "img_test = np.array(img_test); lbl_test = np.array(lbl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "Sneeking at a random image to gain confidence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "show(img_test[8])\n",
    "print(lbl_test[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wew... looks like it all worked out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_train = np.eye(10)[lbl_train]\n",
    "lbl_test = np.eye(10)[lbl_test]\n",
    "lbl_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = img_train / 255\n",
    "img_test = img_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MLFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        z[z < 0] = 0\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        z[z < 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "        \n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        return Sigmoid.activation(z) * (1 - Sigmoid.activation(z))\n",
    "    \n",
    "class MSE:\n",
    "    def __init__(self, activation_fn=None):\n",
    "        self.activation_fn = activation_fn\n",
    "            \n",
    "    def activation(self, z):\n",
    "        return self.activation_fn.activation(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative(y_true, y_pred):\n",
    "        return y_pred - y_true\n",
    "\n",
    "    def delta(self, y_true, y_pred):\n",
    "        return self.derivative(y_true, y_pred) * self.activation_fn.derivative(y_pred)\n",
    "    \n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, dimensions, activation_fns):\n",
    "        self.dimensions = dimensions\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = None\n",
    "        self.learning_rate = None\n",
    "        self.weights = {}\n",
    "        self.bais = {}\n",
    "        self.activations = {}\n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.weights[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])\n",
    "            self.bais[i + 1] = np.zeros(dimensions[i + 1])\n",
    "            self.activations[i + 2] = activation_fns[i]\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        deepcopy_method = self.__deepcopy__\n",
    "        self.__deepcopy__ = None\n",
    "        cp = deepcopy(self, memo)\n",
    "        self.__deepcopy__ = deepcopy_method\n",
    "        # custom treatments\n",
    "        cp.weights = {}; cp.bais = {}\n",
    "        for i in range(cp.n_layers - 1):\n",
    "            cp.weights[i + 1] = np.random.randn(cp.dimensions[i], cp.dimensions[i + 1]) / np.sqrt(cp.dimensions[i])\n",
    "            cp.bais[i + 1] = np.zeros(cp.dimensions[i + 1])\n",
    "\n",
    "        return cp\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        z = {}\n",
    "        activated = {1: x}\n",
    "        for i in range(1, self.n_layers):\n",
    "            z[i + 1] = np.dot(activated[i], self.weights[i]) + self.bais[i]\n",
    "            activated[i + 1] = self.activations[i + 1].activation(z[i + 1])\n",
    "        return z, activated\n",
    "    \n",
    "    def back_propagation(self, z, a, y_true):\n",
    "        delta = self.loss.delta(y_true, a[self.n_layers])\n",
    "        partial_derivative = np.dot(a[self.n_layers - 1].T, delta)\n",
    "\n",
    "        update_params = {\n",
    "            self.n_layers - 1: (partial_derivative, delta)\n",
    "        }\n",
    "\n",
    "        for i in reversed(range(2, self.n_layers)):\n",
    "            delta = np.dot(delta, self.weights[i].T) * self.activations[i].derivative(z[i])\n",
    "            partial_derivative = np.dot(a[i - 1].T, delta)\n",
    "            update_params[i - 1] = (partial_derivative, delta)\n",
    "\n",
    "        for key, values in update_params.items():\n",
    "            self.update_fn(key, values[0], values[1])\n",
    "        \n",
    "    def update_fn(self, key, partial_derivative, delta):\n",
    "        self.weights[key] -= self.learning_rate * partial_derivative\n",
    "        self.bais[key] -= self.learning_rate * np.mean(delta, 0)\n",
    "\n",
    "    def learn(self, x, y_true, loss, epochs, batch_size, learning_rate):\n",
    "        self.loss = loss(self.activations[self.n_layers])\n",
    "        self.learning_rate = learning_rate\n",
    "        for i in range(epochs):\n",
    "            seed = np.arange(x.shape[0])\n",
    "            np.random.shuffle(seed)\n",
    "            x_ = x[seed]\n",
    "            y_ = y_true[seed]\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                k = j * batch_size\n",
    "                l = (j + 1) * batch_size\n",
    "                z, a = self.feed_forward(x_[k:l])\n",
    "                self.back_propagation(z, a, y_[k:l])\n",
    "            _, _a = self.feed_forward(x)\n",
    "            print(\"Epoch:\", i + 1, \"Loss:\", self.loss.loss(y_true, _a[self.n_layers]), end='\\r')\n",
    "    \n",
    "    def predict(self, x):\n",
    "        _, a = self.feed_forward(x)\n",
    "        return a[self.n_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class allows us to create a network pf arbitary size and supports ReLU and Sigmoid as activations functions.\n",
    "\n",
    "Cross-validation is used to determine the better model for this problem, the value of k is 5, i.e. we create 5 splits of the data set. We then will use the results obtained model contructed in each fold to find the better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(img_train)\n",
    "Y = np.copy(lbl_train)\n",
    "\n",
    "# Creating the 5 fold cross-validation \n",
    "kf = KFold(n_splits=5)\n",
    "\"\"\"\n",
    "We create a new model in each fold and train on 4 splits while we hold the 5th split for testing. We repeat this \n",
    "process for all the combinations. We store the accuracy for each split and discard the model. The model with \n",
    "better accuracy will the better suited for our problem.\n",
    "\"\"\"\n",
    "\n",
    "# Define the models\n",
    "\"\"\"This neural network has 3 layers, 784 input neurons, 100 in the hidden layer, and 10 in the output layer.\n",
    "We use a learning rate of 0.01 and a modest 100 epochs to get a rough idea aboyt the model\"\"\"\n",
    "nn1 = NeuralNetwork((784, 100, 10), (Relu, Sigmoid))\n",
    "\n",
    "\"\"\"This neural network has 4 layers, 784 input neurons, 64, 64 in the hidden layers, and 10 in the output layer.\n",
    "We use a learning rate of 0.1 and a modest 100 epochs to get a rough idea aboyt the model\"\"\"\n",
    "nn2 = NeuralNetwork((784, 64, 64, 10), (Sigmoid, Sigmoid, Sigmoid))\n",
    "\n",
    "## The error array is used to hold the errors made in each fold.\n",
    "e1 = []; e2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  1\n",
      "Epoch: 100 Loss: 1.1687428501563933e-05\n",
      "Split:  2\n",
      "Epoch: 2 Loss: 0.0010179748221904128\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-99aa6a5fe2fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Split: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mnn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-93f345727f24>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, x, y_true, loss, epochs, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-93f345727f24>\u001b[0m in \u001b[0;36mfeed_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mactivated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbais\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mactivated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for train, test in kf.split(X):\n",
    "    x = X[train]; y = Y[train]\n",
    "    x_ = X[test]; y_ = Y[test]\n",
    "    print(\"Split: \", i)\n",
    "    i += 1\n",
    "    nn1.learn(x, y, MSE, 100, 128, 0.01)\n",
    "    print()\n",
    "    y_pred = np.argmax(nn1.predict(x_), axis=1)\n",
    "    y_true = np.argmax(y_, axis=1)\n",
    "    e1.append(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error of model 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = np.array(e1)\n",
    "print(\"The mean error of the test-train split:\", 1 - e1.mean())\n",
    "print(\"The standard deviation of the test-train split\", e1.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error of model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = np.array(e2)\n",
    "print(\"The mean error of the test-train split:\", 1 - e2.mean())\n",
    "print(\"The standard deviation of the test-train split\", e2.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = np.array(e1); e2 = np.array(e2)\n",
    "print(\"Avg. accuracy of Model 1:\", e1.mean(), \"\\nAvg. accuracy of Model 2:\", e2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model one just, barely, performs better. We therefore choose, the first model to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Digit classifier\n",
    "\n",
    "We have determined that the neural network #2 is the better one to perform classification. We will now train it to on the entire dataset.\n",
    "\n",
    "We use each pixel as a feature to train the network. This results in a network that takes $28\\times28$ number of pixels as input. We have two hidden layers each with 64 nuerons, activated by a Sigmoid function. Lastly, the output layer has 10 neuron which determine the class label of a given input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the better model\n",
    "if e1.mean() < e2.mean(): \n",
    "    nn_simple = deepcopy(nn1)\n",
    "else:\n",
    "    nn_simple = deepcopy(nn2)\n",
    "\n",
    "# Train the network\n",
    "nn_simple.learn(img_train, lbl_train, MSE, 500, 128, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some metrics of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mak epredictions on the test set\n",
    "y_pred = np.argmax(nn_simple.predict(img_test), axis=1)\n",
    "# Get the true labels\n",
    "y_true = np.argmax(lbl_test, axis=1)\n",
    "print(\"Metrics of Performance\")\n",
    "print(\"Accuracy: \", accuracy_score(y_true, y_pred) * 100, \"%\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"\\n\\nConfusion Matrix\\n\")\n",
    "print(confusion_matrix(y_pred, y_true))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"\\n\\nOther metrics\\n\")\n",
    "print(classification_report(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have scored an accuracy of about 98%. From the precision column we note that all classes have high precision. This is also evident from the consfusion matrix.\n",
    "\n",
    "With more epochs, it seems like there is a good chance of overfitting the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
